{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://drive.google.com/file/d/1-lmvLqHRoVztabnwQ8RbZuDhpsd1kmYY/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1kpc8L4BIgP"
   },
   "source": [
    "**Research project** for the course of *Selected Topics in Music and Acoustic Engineering* :\n",
    "\n",
    "***Music Instrument Classification***\n",
    "---\n",
    "Team:\n",
    "* Andrea Crisafulli\n",
    "* Marco Porcella\n",
    "* Giacomo De Toni\n",
    "* Gianluigi Vecchini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV3R57F_h9nz"
   },
   "source": [
    "### *Import libraries*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1741789150083,
     "user": {
      "displayName": "gianluigi vecchini",
      "userId": "14572864369043101278"
     },
     "user_tz": -60
    },
    "id": "XSbySaOmh9n0"
   },
   "outputs": [],
   "source": [
    "# ✅ Core Python & Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import scipy.signal as signal\n",
    "\n",
    "# ✅ Audio Processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "\n",
    "# ✅ Scikit-learn (ML + preprocessing)\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ✅ TensorFlow / Keras (DL)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ✅ PyTorch (opzionale, se usi anche torch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ✅ Plotting style\n",
    "plt.style.use(\"seaborn-v0_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MFCC(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(32 * 30 * 3, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x.unsqueeze(1))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, n_mfcc=13, max_len=130):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc88qhCUSlQi"
   },
   "source": [
    "### Import audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "csv_path = \".\\ESC-50-master\\_meta\\esc50.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Base path to audio files\n",
    "audio_base_path = Path(\".\\ESC-50-master\\_audio\")\n",
    "filepaths = df[\"filename\"].apply(lambda x: audio_base_path / x)\n",
    "labels = df[\"category\"]\n",
    "\n",
    "# Loading file\n",
    "y, sr = librosa.load(filepaths[0], sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1400 samples\n",
      "Validation: 300 samples\n",
      "Test: 300 samples\n"
     ]
    }
   ],
   "source": [
    "X = filepaths\n",
    "y = labels\n",
    "\n",
    "# Prima split: train + (val+test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
    "\n",
    "# Seconda split: val + test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iNJ-d4zDUwAl"
   ],
   "provenance": [
    {
     "file_id": "1eg4ysGRmcU-8BENj7XWR0P7goV391s_I",
     "timestamp": 1741182208526
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv-timbre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
