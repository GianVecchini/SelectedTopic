{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://drive.google.com/file/d/1-lmvLqHRoVztabnwQ8RbZuDhpsd1kmYY/view?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1kpc8L4BIgP"
   },
   "source": [
    "# TIMBRE\n",
    "\n",
    "**Research project** for the course of *Selected Topics in Music and Acoustic Engineering* :\n",
    "\n",
    "***Music Instrument Classification***\n",
    "\n",
    "This project addresses the development of a system for automatic music instrument classification. The dataset\n",
    "provided is the MedleyDBcollection1, which contains 196 professionally recorded multitrack recordings, including\n",
    "individual stems corresponding to isolated instruments.\n",
    " \n",
    "Students are tasked with designing a classification pipeline that either recognizes instruments in multitimbral mix\n",
    "tures or classifies individual stems where typically one instrument is active. The project encourages a flexible\n",
    "approach, allowing exploration of both isolated and polyphonic scenarios.\n",
    "\n",
    "Key aspects to investigate include:\n",
    "- Analyzing the robustness of instrument recognition systems when facing different levels of overlapping in\n",
    "struments within a mixture.\n",
    "- Studying the relationship between instrumentation and musical genre, as genre annotations are also available in the dataset.\n",
    "- Exploring the use of co-occurrence matrices to model and understand typical combinations of instruments within different musical contexts.\n",
    "The students should experiment with feature extraction techniques sensitive to timbral characteristics, such as\n",
    "spectral descriptors and MFCCs, and assess the effectiveness of classification\n",
    "\n",
    "### Team:\n",
    "* Andrea Crisafulli\n",
    "* Marco Porcella\n",
    "* Giacomo De Toni\n",
    "* Gianluigi Vecchini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV3R57F_h9nz"
   },
   "source": [
    "## *Import libraries*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1741789150083,
     "user": {
      "displayName": "gianluigi vecchini",
      "userId": "14572864369043101278"
     },
     "user_tz": -60
    },
    "id": "XSbySaOmh9n0"
   },
   "outputs": [],
   "source": [
    "# === Core Python & Scientific Computing ===\n",
    "import numpy as np                # Numerical computing\n",
    "import pandas as pd              # Data handling and manipulation\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from pathlib import Path         # File path handling\n",
    "import scipy.signal as signal    # Signal processing tools\n",
    "\n",
    "# === Audio Processing ===\n",
    "import librosa                   # Audio analysis\n",
    "import librosa.display           # Visualization for librosa outputs\n",
    "import IPython.display as ipd    # For audio playback in notebooks\n",
    "\n",
    "# === Scikit-learn: ML & Preprocessing ===\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay  # Evaluation\n",
    "from sklearn.decomposition import PCA         # Dimensionality reduction\n",
    "from sklearn.preprocessing import scale, StandardScaler, MultiLabelBinarizer  # Data scaling & encoding\n",
    "from sklearn.model_selection import train_test_split  # Dataset splitting\n",
    "from sklearn.svm import SVC                    # Support Vector Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # k-NN classifier\n",
    "from sklearn.cluster import KMeans            # Clustering\n",
    "\n",
    "# === Deep Learning: TensorFlow / Keras ===\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint  # Training utilities\n",
    "from keras.optimizers import Adam            # Optimizer for model training\n",
    "from keras import layers, models\n",
    "\n",
    "# === Optional: PyTorch (if used) ===\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F              # Functional API for building models\n",
    "\n",
    "# === Others ===\n",
    "import yaml                                   # Parsing metadata in YAML format\n",
    "from collections import Counter               # Frequency counting for label analysis\n",
    "from tqdm import tqdm                         # Progress bar for loops\n",
    "\n",
    "# === Plotting Style ===\n",
    "#plt.style.use(\"seaborn-v0_8\")                 # Set default plotting style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXECUTION SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for data extraction\n",
    "considerMixFiles = True\n",
    "considerStemFiles = True\n",
    "considerRawFiles = False\n",
    "\n",
    "# Variable for OS recognition\n",
    "OSys = 0  # 0 = WINDOWS, 1 = MACOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since 83 different labels are present we can group togheter similar labels\n",
    "labelGroupsDict = {\n",
    "    'Main System': 'Main System',               # 12\n",
    "\n",
    "    # Vocals \n",
    "    'male singer': 'vocals',                    # 82\n",
    "    'female singer': 'vocals',                  # 57\n",
    "    'male rapper': 'vocals',                    # 8\n",
    "    'male speaker': 'vocals',                   # 2\n",
    "    'vocalists': 'vocals',                      # 60 \n",
    "\n",
    "    # Guitar\n",
    "    'acoustic guitar': 'guitar',                # 50\n",
    "    'clean electric guitar': 'guitar',          # 94\n",
    "    'distorted electric guitar': 'guitar',      # 54\n",
    "    'lap steel guitar': 'guitar',               \n",
    "\n",
    "    # Small guitars\n",
    "    'liuqin': 'similar guitars',               \n",
    "    'banjo': 'similar guitars',\n",
    "    'mandolin': 'similar guitars',              # 18\n",
    "    'oud':'similar guitars',\n",
    "    'zhongruan':'similar guitars',\n",
    "\n",
    "    # Electric bass\n",
    "    'electric bass': 'electric bass',           # 126\n",
    "\n",
    "    # Violin\n",
    "    'violin': 'violin',                         # 41\n",
    "    'violin section': 'violin',                 # 28\n",
    "\n",
    "    # Viola\n",
    "    'viola': 'viola',                           # 15\n",
    "    'viola section': 'viola',                   # 8\n",
    "\n",
    "    # Cello\n",
    "    'cello': 'cello',                           # 24\n",
    "    'cello section': 'cello',\n",
    "\n",
    "    # double bass\n",
    "    'double bass': 'strings',                   # 33\n",
    "\n",
    "    # string section\n",
    "    'string section': 'strings',                # 12\n",
    "    'erhu': 'strings',                          # 12\n",
    "\n",
    "    # Trumpet\n",
    "    'trumpet': 'brass',                         # 15\n",
    "    'trumpet section': 'brass',\n",
    "\n",
    "    # Trombone\n",
    "    'trombone': 'trombone',\n",
    "    'trombone section': 'trombone',\n",
    "\n",
    "    # Trombone\n",
    "    'french horn': 'french horn',\n",
    "    'french horn section': 'french horn',\n",
    "\n",
    "    # Tuba\n",
    "    'tuba': 'tuba',\n",
    "\n",
    "    # Brass\n",
    "    'horn section': 'brass',\n",
    "    'brass section': 'brass',                   # 16\n",
    "\n",
    "    # Saxophone\n",
    "    'saxophone': 'saxophone',\n",
    "    'soprano saxophone': 'saxophone',\n",
    "    'alto saxophone': 'saxophone',\n",
    "    'tenor saxophone': 'saxophone',\n",
    "    'baritone saxophone': 'saxophone',\n",
    "\n",
    "    # Woodwinds\n",
    "    'dizi': 'woodwinds',\n",
    "    'flute': 'woodwinds',                       # 22\n",
    "    'flute section': 'woodwinds',\n",
    "    'piccolo': 'woodwinds',\n",
    "    'clarinet': 'woodwinds',                    # 17\n",
    "    'clarinet section': 'woodwinds',\n",
    "    'bass clarinet': 'woodwinds',\n",
    "    'oboe': 'woodwinds',\n",
    "    'bassoon': 'woodwinds',                     # 11\n",
    "    'bamboo flute' : 'woodwinds',\n",
    "\n",
    "    # Drum set\n",
    "    'drum set': 'drum set',                     # 131\n",
    "    'snare drum': 'drum set',\n",
    "    'kick drum': 'drum set',\n",
    "    'bass drum': 'drum set',\n",
    "\n",
    "    # Percussion\n",
    "    'toms': 'percussion',\n",
    "    'doumbek': 'percussion',\n",
    "    'tabla': 'percussion',                      # 27\n",
    "    'darbuka': 'percussion',\n",
    "    'tambourine': 'percussion',\n",
    "    'shaker': 'percussion',\n",
    "    'bongo': 'percussion',\n",
    "    'cymbal': 'percussion',                     # 14\n",
    "    'timpani': 'percussion',                    # 13\n",
    "    'auxiliary percussion': 'percussion',       # 36\n",
    "    'claps': 'percussion',\n",
    "    'gong': 'percussion',\n",
    "    'gu':'percussion',\n",
    "    'drum machine': 'percussion',                 # 32\n",
    "    'scratches': 'percussion',\n",
    "\n",
    "    # Xilofono\n",
    "    'chimes': 'xilofono',\n",
    "    'glockenspiel':'xilofono',\n",
    "    'vibraphone':'xilofono',                    # 16\n",
    "\n",
    "    # Harps\n",
    "    'guzheng':'harps',\n",
    "    'harp':'harps',\n",
    "    'yangqin':'harps',                          # 12\n",
    "\n",
    "    # Piano\n",
    "    'piano': 'piano',                           # 86\n",
    "    'electric piano': 'piano', \n",
    "\n",
    "    # Accordion\n",
    "    'accordion': 'accordion',                   # 10\n",
    "\n",
    "    # Harmonica\n",
    "    'harmonica': 'harmonica',                   # 4\n",
    "    \n",
    "    # Keyboard and piano\n",
    "    'tack piano': 'keyboard',                   # 18\n",
    "    'melodica': 'keyboard',         \n",
    "    'sampler': 'keyboard',\n",
    "    'synthesizer': 'keyboard',                  # 74 \n",
    "\n",
    "    # fx/processed sound\n",
    "    'fx/processed sound':'fx/processed sound',  # 63 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc88qhCUSlQi"
   },
   "source": [
    "### *Import audio data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Impossibile trovare il percorso specificato: 'E:\\\\MedleyDB\\\\Audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (considerMixFiles \u001b[38;5;129;01mor\u001b[39;00m considerStemFiles \u001b[38;5;129;01mor\u001b[39;00m considerRawFiles \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m errorPath):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Iterates over directories in the melodyDB/Audio folder\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m songDir \u001b[38;5;129;01min\u001b[39;00m audioPath\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[0;32m     18\u001b[0m         labelArray \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Security check to skip not directory items\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[0;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[0;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: 'E:\\\\MedleyDB\\\\Audio'"
     ]
    }
   ],
   "source": [
    "windows = 0\n",
    "macOS = 1\n",
    "errorPath = False\n",
    "\n",
    "if OSys == windows:\n",
    "    basePath = Path(\"E:/MedleyDB\")                         # For windows\n",
    "elif OSys == macOS:\n",
    "    basePath = Path(\"/Volumes/Extreme SSD/MedleyDB\")        # For mac\n",
    "else:\n",
    "    errorPath = True\n",
    "\n",
    "audioPath = basePath / \"Audio\"\n",
    "data = []\n",
    "\n",
    "if (considerMixFiles or considerStemFiles or considerRawFiles and not errorPath):\n",
    "    # Iterates over directories in the melodyDB/Audio folder\n",
    "    for songDir in audioPath.iterdir():\n",
    "        labelArray = []\n",
    "        \n",
    "        # Security check to skip not directory items\n",
    "        if not songDir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        songName = songDir.name\n",
    "        yamlFilePath = audioPath / songDir / f\"{songName}_METADATA.yaml\" # Path to YAML metadata file\n",
    "        \n",
    "        # Opens YAML metadata file in read mode\n",
    "        with open(yamlFilePath, \"r\") as f:\n",
    "            metadata = yaml.safe_load(f)\n",
    "        \n",
    "        # Recovers stems from metadata and stores in dictionary\n",
    "        stemsData = metadata.get(\"stems\", {})\n",
    "        \n",
    "        # Iterates over stems\n",
    "        for stemId, stem in stemsData.items():\n",
    "            instrumentData = [] # Empty data for raw paths\n",
    "            \n",
    "            rawData = stem.get(\"raw\", {})\n",
    "            \n",
    "            # import Raw files (SKIP FOR NOW)\n",
    "            if considerRawFiles:\n",
    "                # Iterates over raw items to store the relative paths\n",
    "                for rawId, raw in rawData.items():\n",
    "                    rawPath = songDir /  f\"{songName}_RAW\" / raw.get(\"filename\")\n",
    "                    \n",
    "                    # Checks for valid files\n",
    "                    if(not rawPath.name.startswith(\".\")):\n",
    "                        rawData = {\n",
    "                            \"song\": songName,\n",
    "                            \"songPath\": audioPath / songDir,\n",
    "                            \"label\": labelGroupsDict[stem.get(\"instrument\")],\n",
    "                            \"filePath\": rawPath\n",
    "                        }\n",
    "                        \n",
    "                        data.append(rawData)\n",
    "                \n",
    "            # Creates new data\n",
    "            if considerStemFiles:\n",
    "                stemData = {\n",
    "                    \"song\": songName,\n",
    "                    \"songPath\": songDir,\n",
    "                    \"label\": labelGroupsDict[stem.get(\"instrument\")],\n",
    "                    \"filePath\": songDir / f\"{songName}_STEMS\" / stem.get(\"filename\")\n",
    "                }\n",
    "                \n",
    "                # Appends to data \n",
    "                data.append(stemData)\n",
    "            \n",
    "            if considerMixFiles:\n",
    "                labelArray.append(labelGroupsDict[stem.get(\"instrument\")])\n",
    "        \n",
    "        if considerMixFiles:\n",
    "            # Format\n",
    "            labelArray = np.unique(labelArray)\n",
    "            labelFormatted = \"|\".join(sorted(set(labelArray)))\n",
    "            \n",
    "            mixData = {\n",
    "                \"song\": songName,\n",
    "                \"songPath\": audioPath / songDir,\n",
    "                \"label\": labelFormatted,\n",
    "                \"filePath\": songDir / f\"{songName}_MIX.wav\"\n",
    "            }\n",
    "            \n",
    "            data.append(mixData)\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} audio files.\")\n",
    "\n",
    "    # String convertion to list\n",
    "    df[\"labelList\"] = df[\"label\"].str.split(\"|\")\n",
    "\n",
    "    mlbAllDataset = MultiLabelBinarizer()\n",
    "    audioLabelsBinary = mlbAllDataset.fit_transform(df[\"labelList\"])\n",
    "    audioLabelsBinary = np.asarray(audioLabelsBinary)\n",
    "elif errorPath:\n",
    "    print(\"ERROR: defined OS is not supported.\")\n",
    "else:\n",
    "    print(\"ERROR: no data to extract with current setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Head of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail of dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info of dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra tutte le righe e colonne senza troncamento\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "df_labels = pd.DataFrame(audioLabelsBinary, columns=mlbAllDataset.classes_)\n",
    "df_labels.sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing variables\n",
    "audioFiles = []\n",
    "audioLabels = []\n",
    "\n",
    "# Extract paths and labels\n",
    "for _, row in df.iterrows():\n",
    "    audioFiles.append(row[\"filePath\"])\n",
    "    audioLabels.append(row[\"labelList\"])\n",
    "    \n",
    "\n",
    "# Security check\n",
    "if(len(audioFiles) == len(audioLabels)):\n",
    "    print(f\"Extracted files and labels for a total lenght of {len(audioFiles)}\")\n",
    "else:\n",
    "    print(\"Error in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of classes to extract to form the partial dataset (MAX = 82)\n",
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all labels e conta frequenze\n",
    "all_labels = sum(df[\"labelList\"], [])\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "if n_classes <= label_counts.total():\n",
    "    # Prendi le 10 classi più frequenti\n",
    "    top_labels = [label for label, _ in label_counts.most_common(n_classes)]\n",
    "    print(\"Top labels:\", top_labels)\n",
    "\n",
    "    # Filtra righe dove almeno una label è in top_labels\n",
    "    df_subset = df[df[\"labelList\"].apply(lambda labels: any(label in top_labels for label in labels))]\n",
    "\n",
    "    # Estrai audio e label\n",
    "    audioFilesSubset = df_subset[\"filePath\"].tolist()\n",
    "    audioLabelsSubset = df_subset[\"labelList\"].tolist()\n",
    "\n",
    "    mlbPartialDataset = MultiLabelBinarizer(classes=top_labels)\n",
    "    audioLabelsSubsetBinary = mlbPartialDataset.fit_transform(df_subset[\"labelList\"])\n",
    "    audioLabelsSubsetBinary = np.asarray(audioLabelsSubsetBinary)\n",
    "\n",
    "    if len(audioFilesSubset) == len(audioLabelsSubset):\n",
    "        print(f\"Extracted {len(audioFilesSubset)} samples from top {n_classes} labels\")\n",
    "    else:\n",
    "        print(\"Mismatch in extracted data\")\n",
    "\n",
    "    print(f\"Subset binary label matrix shape: {audioLabelsSubsetBinary.shape}\")\n",
    "else:\n",
    "    print(f\"ERROR: selected n_classes of {n_classes} exceeds the total number of classes in DF ({label_counts})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executionMode = 1   # 0 = all dataset, 1 = partial dataset\n",
    "\n",
    "allDataset = 0\n",
    "partialDataset = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if elif block to choose execution mode between all dataset and partial dataset\n",
    "if executionMode == allDataset:\n",
    "    mlb = mlbAllDataset                  # mlb to use\n",
    "    labelsToLoad = audioLabelsBinary     # audioLabels to use\n",
    "    audioFilesToExtract = audioFiles     # audio files to load\n",
    "elif executionMode == partialDataset:\n",
    "    mlb = mlbPartialDataset\n",
    "    labelsToLoad = audioLabelsSubsetBinary\n",
    "    audioFilesToExtract = audioFilesSubset\n",
    "\n",
    "signals = []\n",
    "\n",
    "timeExtraction = 10  # in seconds\n",
    "samplingRate = 22050\n",
    "num_samples = int(samplingRate * timeExtraction)\n",
    "minAmplitude = 0.5\n",
    "\n",
    "# Extraction of files via librosa load (TQDM to show progress)\n",
    "for x in tqdm(audioFilesToExtract, desc=\"Loading audio files...\"):\n",
    "    y, _ = librosa.load(x, sr=samplingRate)\n",
    "    \n",
    "    # Normalizzazione\n",
    "    if np.max(np.abs(y)) > 0:\n",
    "        y = y / np.max(np.abs(y))\n",
    "\n",
    "    # Trova primo indice significativo\n",
    "    if np.any(y > minAmplitude):\n",
    "        start_index = np.argmax(y > minAmplitude)\n",
    "    else:\n",
    "        start_index = 0\n",
    "\n",
    "    # Calcola fine dell’estrazione\n",
    "    end_index = start_index + num_samples\n",
    "    \n",
    "    if end_index <= len(y):\n",
    "        y = y[start_index:end_index]\n",
    "    else:\n",
    "        # Prova a traslare lo start indietro se possibile\n",
    "        if len(y) >= num_samples:\n",
    "            start_index = len(y) - num_samples\n",
    "            y = y[start_index:]\n",
    "        else:\n",
    "            # Troppo corto: pad con zeri alla fine\n",
    "            y = y[start_index:]\n",
    "            padding_needed = num_samples - len(y)\n",
    "            y = np.pad(y, (0, padding_needed), 'constant')\n",
    "\n",
    "    y = y.astype(np.float16)\n",
    "    signals.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melSpegrams = []\n",
    "\n",
    "# Iterates over signals, normalizes them and computes mel spectrograms via librosa feature\n",
    "for signal in tqdm(signals, desc=\"Processing audio signals...\"):\n",
    "\n",
    "    # Creation of mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=signal, sr=22050)\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    melSpegrams.append(S_dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of spectrograms\n",
    "import IPython.display\n",
    "\n",
    "for i in range(0, len(signals), int(len(signals)/5)):\n",
    "    print(\"\\n\\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\")\n",
    "    \n",
    "    original_labels = mlb.inverse_transform(np.array([labelsToLoad[i]]))\n",
    "    print(\"Labels:\", original_labels)\n",
    "\n",
    "    IPython.display.display(IPython.display.Audio(signals[i], rate=22050))\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    librosa.display.specshow(melSpegrams[i], sr=22050, x_axis='time', y_axis='mel', fmax=22050/2)\n",
    "    plt.clim(-80,None)\n",
    "    plt.colorbar()\n",
    "\n",
    "    filename = str(audioFilesToExtract[i]).split(\"\\\\\")[-1]\n",
    "    \n",
    "    plt.title(f'{filename} (data #{i})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user now has to choose which data he wants to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melSpegrams = np.asarray(melSpegrams)\n",
    "labelsToLoad = np.asarray(labelsToLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "melSpegrams, labelsToLoad = shuffle(melSpegrams, labelsToLoad, random_state=1234)\n",
    "\n",
    "# Split into Train (70%) and Temp (30%)\n",
    "# X = mel spectrograms\n",
    "# y = label\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "\n",
    "# Convert X to numpy array (es. melSpectrograms devono essere np.array)\n",
    "X = np.array(melSpegrams)\n",
    "y = np.array(labelsToLoad)\n",
    "\n",
    "# Split: Train (70%), Temp (30%)\n",
    "X_train, y_train, X_temp, y_temp = iterative_train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Split: Validation (15%), Test (15%)\n",
    "X_val, y_val, X_test, y_test = iterative_train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "# Summary\n",
    "print(f\"Train samples:      {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples:       {len(X_test)}\")\n",
    "print(f\"Test lables:       {len(y_train)}\")\n",
    "print(f\"Validation lables: {len(y_val)}\")\n",
    "print(f\"Test lables:       {len(y_test)}\")\n",
    "\n",
    "\n",
    "print(\"Train:\", np.sum(y_train, axis=0))\n",
    "print(\"Val:  \", np.sum(y_val, axis=0))\n",
    "print(\"Test: \", np.sum(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = np.sum(y_train, axis=0)\n",
    "val_counts = np.sum(y_val, axis=0)\n",
    "test_counts = np.sum(y_test, axis=0)\n",
    "\n",
    "ruby = '#9B111E'\n",
    "sapphire = '#0F52BA'\n",
    "emerald = '#50C878'\n",
    "\n",
    "labelsChart = mlb.classes_\n",
    "x = np.arange(len(labelsChart))  # posizione per ogni classe\n",
    "width = 0.25  # larghezza delle barre\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Barre\n",
    "plt.bar(x - width, train_counts, width, label='Train', color=ruby)\n",
    "plt.bar(x, val_counts, width, label='Validation', color=sapphire)\n",
    "plt.bar(x + width, test_counts, width, label='Test', color=emerald)\n",
    "\n",
    "# Asse x e titoli\n",
    "plt.xticks(x, labelsChart, rotation=45, ha='right')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Occurrences')\n",
    "plt.title(f'Class distribution in Dataset (Total elements: {np.sum(labelsToLoad)})')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define input shape and number of output classes ===\n",
    "inputShape = (128, melSpegrams[0].shape[1], 1)  # (n_mels, time_frames, channels)\n",
    "numClasses = labelsToLoad.shape[1]              # number of multilabel classes\n",
    "\n",
    "# === Build CNN model ===\n",
    "modelCNN = models.Sequential([\n",
    "\n",
    "    # Input\n",
    "    layers.Input(shape=inputShape),\n",
    "\n",
    "    # === Block 1 ===\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((3, 3), padding='same'),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # === Block 2 ===\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((3, 3), padding='same'),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # === Block 3 ===\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((3, 3), padding='same'),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # === Block 4 ===\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    # === Fully Connected ===\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    # === Output Layer (sigmoid for multilabel) ===\n",
    "    layers.Dense(numClasses, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# === Compile the model ===\n",
    "from keras.metrics import BinaryAccuracy, AUC\n",
    "modelCNN.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[BinaryAccuracy(name='binary_accuracy'), AUC(multi_label=True, name='auc')]\n",
    ")\n",
    "\n",
    "# === Summary ===\n",
    "modelCNN.summary()\n",
    "\n",
    "# Optional: show classes\n",
    "print(f\"Number of classes: {numClasses}\")\n",
    "print(f\"Class names: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save logs and models\n",
    "csvLogPath = 'training_log.csv'\n",
    "checkpointPath = 'best_model.h5'\n",
    "\n",
    "# CSVLogger: logs every epoch to CSV\n",
    "csvLogger = CSVLogger(csvLogPath, append=True)\n",
    "\n",
    "# EarlyStopping: stop if val_loss doesn't improve after 100 epochs\n",
    "earlyStop = EarlyStopping(\n",
    "    monitor='val_binary_accuracy',\n",
    "    patience=100,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ModelCheckpoint: save best model based on val_accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpointPath,\n",
    "    monitor='val_binary_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Bundle them\n",
    "callbacks = [csvLogger, earlyStop, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.metrics import BinaryAccuracy, AUC\n",
    "\n",
    "# Compile the model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "modelCNN.compile(\n",
    "    optimizer=opt,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[BinaryAccuracy(name='binary_accuracy'), AUC(multi_label=True, name='auc')]\n",
    ")\n",
    "\n",
    "batchSize=32\n",
    "epochs=300\n",
    "\n",
    "history = modelCNN.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batchSize, epochs=epochs, verbose=0, callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['binary_accuracy']) \n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('Model binary Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Accuracy')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "#plt.ylim(0, 3);\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "#plt.ylim(0, 2);\n",
    "\n",
    "#best validation accuracy\n",
    "bestValAccuracy = np.max(history.history['val_binary_accuracy'])\n",
    "print('Best validation accuracy: ', bestValAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui evaluate e visualizza il risultato\n",
    "results = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "print(results)  # Stampa i risultati per vedere quanti valori vengono restituiti\n",
    "\n",
    "# Se sono più di 2 valori, puoi selezionare quello che ti interessa\n",
    "testLoss = results[0]\n",
    "testAccuracy = results[1]  # Oppure index corretti se ci sono più metriche\n",
    "\n",
    "print(f\"Test Loss: {testLoss:.4f}\")\n",
    "print(f\"Test Accuracy: {testAccuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN.load_weights('best_model.h5', by_name=False)\n",
    "\n",
    "resultsTest = modelCNN.evaluate(X_test, y_test)\n",
    "print('Test Loss: {} \\nTest Accuracy: {}'.format(resultsTest[0], resultsTest[1]))\n",
    "\n",
    "resultsVal = modelCNN.evaluate(X_val, y_val)\n",
    "print('Val Loss: {} \\nVal Accuracy: {}'.format(resultsVal[0], resultsVal[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set\n",
    "threshold = 0.5\n",
    "y_pred_probs = modelCNN.predict(X_test)                     # returns probabilities for each label (from sigmoid outputs)\n",
    "y_pred_binary = (y_pred_probs > threshold).astype(int)      # if the probability of a class is higher then treshold then consider it active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix considering the MIX as a separate lable:\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Convert y_test back to normal label names or \"MIX\"\n",
    "y_test_simplified = []\n",
    "for row in y_test:\n",
    "    active_indices = np.where(row == 1)[0] # indices of the MLB lable in which the there is a 1 and not a 0\n",
    "    if len(active_indices) == 1:\n",
    "        y_test_simplified.append(mlb.classes_[active_indices[0]]) \n",
    "    else:\n",
    "        y_test_simplified.append(\"MIX\")\n",
    "\n",
    "# Convert y_pred_binary back to normal label names or \"MIX\"\n",
    "y_pred_simplified = []\n",
    "for row in y_pred_binary:\n",
    "    active_indices = np.where(row == 1)[0]\n",
    "    if len(active_indices) == 1:\n",
    "        y_pred_simplified.append(mlb.classes_[active_indices[0]]) \n",
    "    else:\n",
    "        y_pred_simplified.append(\"MIX\")\n",
    "\n",
    "# Print confusion matrix\n",
    "allLabels = list(mlb.classes_) + [\"MIX\"]\n",
    "\n",
    "cm = confusion_matrix(y_test_simplified, y_pred_simplified, labels=allLabels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=allLabels)\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmLimited = cm[40:60,40:60]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmLimited, display_labels=allLabels[40:60])\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "# Confusion matrix plotted for each class, using the multilables\n",
    "conf_matrices = multilabel_confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "class_names = mlb.classes_\n",
    "\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "                yticklabels=[\"True 0\", \"True 1\"])\n",
    "    plt.title(f\"Confusion Matrix for class: {class_names[i]}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"{class_names[i]} — TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred_binary,\n",
    "    target_names=mlb.classes_,\n",
    "    zero_division=0  \n",
    ")\n",
    "\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iNJ-d4zDUwAl"
   ],
   "provenance": [
    {
     "file_id": "1eg4ysGRmcU-8BENj7XWR0P7goV391s_I",
     "timestamp": 1741182208526
    }
   ]
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
